name: DD

on:
  schedule:
    - cron: '30 14 * * *'  # Run daily at 10:30 AM UTC (4:00 PM Indian time - UTC+5:30)
  workflow_dispatch:     # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 1200  # 20 hours timeout
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python 3.12
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.3'
          cache: 'pip'
          check-latest: true

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wget unzip xvfb libxss1 libxtst6 libnss3 libatk1.0-0 \
            libcups2 libxcomposite1 libxdamage1 libxfixes3 libxrandr2 libgbm1 libxkbcommon0 \
            libatspi2.0-0 libx11-xcb1 libasound2t64 libatk-bridge2.0-0

          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

          sudo Xvfb :99 -screen 0 1920x1080x24 > /dev/null 2>&1 &
          echo "CHROME_PATH=$(which google-chrome)" >> $GITHUB_ENV
          echo "DISPLAY=:99" >> $GITHUB_ENV

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install "undetected-chromedriver>=3.5.5" --no-cache-dir
          pip install -r requirements.txt
          python -c "import undetected_chromedriver as uc; print(f'✓ Undetected ChromeDriver version: {uc.__version__}')"

      - name: Setup credentials
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > credentials.json
          python -c "
          import json
          with open('credentials.json', 'r') as f:
              json.load(f)
          print('✓ Google credentials JSON validated')"
          echo CLAUDE_API_KEY=${{ secrets.CLAUDE_API_KEY }} > .env

      - name: Download proxies
        run: |
          python -c "
          import requests, random, time
          sources = [
            'https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt',
            'https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/http.txt',
            'https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt'
          ]
          headers = {'User-Agent': 'Mozilla/5.0'}
          proxies = set()
          for url in sources:
              try:
                  r = requests.get(url, headers=headers, timeout=10)
                  if r.status_code == 200:
                      proxies.update(line.strip() for line in r.text.splitlines() if line.strip())
                      print(f'Downloaded from {url}')
              except Exception as e:
                  print(f'Error: {e}')
              time.sleep(random.uniform(1, 2))
          with open('proxies.txt', 'w') as f:
              for p in proxies: f.write(p + '\\n')
          print(f'Total proxies saved: {len(proxies)}')"

      - name: Run name: Gems

on:
  schedule:
    - cron: '30 13 * * *'  # Run daily at 10:30 AM UTC (4:00 PM Indian time - UTC+5:30)
  workflow_dispatch:     # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 1200  # 20 hours timeout
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python 3.12
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.3'
          cache: 'pip'
          check-latest: true

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wget unzip xvfb libxss1 libxtst6 libnss3 libatk1.0-0 \
            libcups2 libxcomposite1 libxdamage1 libxfixes3 libxrandr2 libgbm1 libxkbcommon0 \
            libatspi2.0-0 libx11-xcb1 libasound2t64 libatk-bridge2.0-0

          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

          sudo Xvfb :99 -screen 0 1920x1080x24 > /dev/null 2>&1 &
          echo "CHROME_PATH=$(which google-chrome)" >> $GITHUB_ENV
          echo "DISPLAY=:99" >> $GITHUB_ENV

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install "undetected-chromedriver>=3.5.5" --no-cache-dir
          pip install -r requirements.txt
          python -c "import undetected_chromedriver as uc; print(f'✓ Undetected ChromeDriver version: {uc.__version__}')"

      - name: Setup credentials
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > credentials.json
          python -c "
          import json
          with open('credentials.json', 'r') as f:
              json.load(f)
          print('✓ Google credentials JSON validated')"
          echo CLAUDE_API_KEY=${{ secrets.CLAUDE_API_KEY }} > .env

      - name: Download proxies
        run: |
          python -c "
          import requests, random, time
          sources = [
            'https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt',
            'https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/http.txt',
            'https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt'
          ]
          headers = {'User-Agent': 'Mozilla/5.0'}
          proxies = set()
          for url in sources:
              try:
                  r = requests.get(url, headers=headers, timeout=10)
                  if r.status_code == 200:
                      proxies.update(line.strip() for line in r.text.splitlines() if line.strip())
                      print(f'Downloaded from {url}')
              except Exception as e:
                  print(f'Error: {e}')
              time.sleep(random.uniform(1, 2))
          with open('proxies.txt', 'w') as f:
              for p in proxies: f.write(p + '\\n')
          print(f'Total proxies saved: {len(proxies)}')"

      - name: Run Ad_details_scraper.py
        env:
          DISPLAY: ':99'
          CHROME_PATH: /usr/bin/google-chrome
        run: |
          set -e
          echo "STARTING SCRIPT..."
          python Ad_details_scraper.py || echo "⚠️ Script crashed but continuing workflow"

      - name: Upload scraping results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraping-results
          path: |
            ads_data.json
            logs/

      - name: Upload screenshots and debug HTMLs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-files
          path: |
            *.png
            *.html
        env:
          DISPLAY: ':99'
          CHROME_PATH: /usr/bin/google-chrome
        run: |
          set -e
          echo "STARTING SCRIPT..."
          python Ad_details_scraper.py || echo "⚠️ Script crashed but continuing workflow"

      - name: Upload scraping results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraping-results
          path: |
            ads_data.json
            logs/

      - name: Upload screenshots and debug HTMLs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-files
          path: |
            *.png
            *.html
