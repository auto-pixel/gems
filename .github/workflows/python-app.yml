name: Gems 
on:
  schedule:
    - cron: '30 13 * * *'  # Run daily at 10:30 AM UTC (4:00 PM Indian time - UTC+5:30)
  workflow_dispatch:     # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 1200  # 20 hours timeout for long-running tasks
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python 3.12
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.3'
          cache: 'pip'
          check-latest: true
      
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wget unzip xvfb
          
          # Install Chrome
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          
          # Verify installations
          google-chrome --version
          
          # Set up virtual display
          sudo Xvfb :99 -screen 0 1920x1080x24 > /dev/null 2>&1 &
          export DISPLAY=:99
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install undetected-chromedriver>=3.5.5
      
      - name: Setup credentials
        run: |
          # Create Google credentials file from GitHub secrets
          # Single quotes around the secret prevent shell interpretation issues
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > credentials.json
          # Make sure the JSON is properly formatted without any extra characters
          python -c "
          import json, sys, os
          try:
              # Try to parse the credentials file
              with open('credentials.json', 'r') as f:
                  json.load(f)
              print('âœ“ Google credentials JSON validated successfully')
          except json.JSONDecodeError as e:
              print(f'ERROR: Invalid JSON in credentials file: {e}')
              print('Checking file content:')
              with open('credentials.json', 'r') as f:
                  print(f.read())
              sys.exit(1)
          "
          
          # Create .env file with Claude API key (no quotes needed)
          echo CLAUDE_API_KEY=${{ secrets.CLAUDE_API_KEY }} > .env
          
          echo "Credentials files created successfully"
        
      - name: Download proxies
        run: |
          # Download fresh proxies from free proxy list
          python -c "
          import requests
          import random
          import time
          
          user_agents = [
              'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
              'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0',
              'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Safari/605.1.15'
          ]
          
          headers = {'User-Agent': random.choice(user_agents)}
          
          # Try multiple sources for redundancy
          sources = [
              'https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt',
              'https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/http.txt',
              'https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt'
          ]
          
          proxies = set()
          
          for source in sources:
              try:
                  response = requests.get(source, headers=headers, timeout=10)
                  if response.status_code == 200:
                      new_proxies = [line.strip() for line in response.text.splitlines() if line.strip()]
                      proxies.update(new_proxies)
                      print(f'Downloaded {len(new_proxies)} proxies from {source}')
                  time.sleep(random.uniform(1, 3))  # Be nice to the servers
              except Exception as e:
                  print(f'Error downloading from {source}: {e}')
          
          # Write to file
          with open('proxies.txt', 'w') as f:
              for proxy in proxies:
                  f.write(f'{proxy}\\n')
          
          print(f'Saved {len(proxies)} unique proxies to proxies.txt')
          "
      

      - name: Run master script
        env:
          DISPLAY: ':99'
        run: |
          # Set display for headless Chrome
          export DISPLAY=:99
          Xvfb :99 -screen 0 1920x1080x24 > /dev/null 2>&1 &
          
          # Run the script with a longer timeout
          python master_script.py
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: scraping-results
          path: |
            ads_data.json
            logs/
