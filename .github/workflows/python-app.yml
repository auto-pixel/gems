name: gemns

on:
  schedule:
    - cron: '30 13 * * *'  # Run daily at 10:30 AM UTC (4:00 PM Indian time - UTC+5:30)
  workflow_dispatch:     # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 720 # Set appropriate timeout
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python 3.12
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.3'
          cache: 'pip'
          check-latest: true
      
      - name: Install Firefox and GeckoDriver
        run: |
          sudo apt-get update
          sudo apt-get install -y firefox wget
          
          # Download and install GeckoDriver
          wget https://github.com/mozilla/geckodriver/releases/download/v0.33.0/geckodriver-v0.33.0-linux64.tar.gz
          sudo tar -xzf geckodriver-*-linux64.tar.gz -C /usr/local/bin
          sudo chmod +x /usr/local/bin/geckodriver
          export PATH="/usr/local/bin:$PATH"
          geckodriver --version
          firefox --version
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          # Temporarily remove firefox-geckodriver from requirements
          grep -v "firefox-geckodriver" requirements.txt > temp_requirements.txt
          pip install -r temp_requirements.txt
      
      - name: Setup credentials
        run: |
          # Create Google credentials file from GitHub secrets
          # Single quotes around the secret prevent shell interpretation issues
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > credentials.json
          # Make sure the JSON is properly formatted without any extra characters
          python -c "
          import json, sys, os
          try:
              # Try to parse the credentials file
              with open('credentials.json', 'r') as f:
                  json.load(f)
              print('âœ“ Google credentials JSON validated successfully')
          except json.JSONDecodeError as e:
              print(f'ERROR: Invalid JSON in credentials file: {e}')
              print('Checking file content:')
              with open('credentials.json', 'r') as f:
                  print(f.read())
              sys.exit(1)
          "
          
          # Create .env file with Claude API key (no quotes needed)
          echo CLAUDE_API_KEY=${{ secrets.CLAUDE_API_KEY }} > .env
          
          echo "Credentials files created successfully"
        
      - name: Download proxies
        run: |
          # Download fresh proxies from free proxy list
          python -c "
          import requests
          import random
          import time
          
          user_agents = [
              'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
              'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0',
              'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Safari/605.1.15'
          ]
          
          headers = {'User-Agent': random.choice(user_agents)}
          
          # Try multiple sources for redundancy
          sources = [
              'https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt',
              'https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/http.txt',
              'https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt'
          ]
          
          proxies = set()
          
          for source in sources:
              try:
                  response = requests.get(source, headers=headers, timeout=10)
                  if response.status_code == 200:
                      new_proxies = [line.strip() for line in response.text.splitlines() if line.strip()]
                      proxies.update(new_proxies)
                      print(f'Downloaded {len(new_proxies)} proxies from {source}')
                  time.sleep(random.uniform(1, 3))  # Be nice to the servers
              except Exception as e:
                  print(f'Error downloading from {source}: {e}')
          
          # Write to file
          with open('proxies.txt', 'w') as f:
              for proxy in proxies:
                  f.write(f'{proxy}\\n')
          
          print(f'Saved {len(proxies)} unique proxies to proxies.txt')
          "
      
      - name: Install ffmpeg
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg
          # Verify installation
          ffmpeg -version
          
      - name: Run master script
        run: python Claude_ai.py
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: scraping-results
          path: |
            ads_data.json
            logs/
